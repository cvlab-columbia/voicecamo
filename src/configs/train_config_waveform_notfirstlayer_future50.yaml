optim:
  name: 'SGD'
  learning_rate: 1.5e-4  # Initial Learning Rate
  learning_anneal: 0.99  # Annealing applied to learning rate after each epoch
  weight_decay: 0.00001
  eps: 1e-8  # Adam eps
  betas: (0.9, 0.999)
SEED: 123456

wandb: True
waveform: True
firstlayer: False
capped: True
powerpenalization: 0.0
inputreal: False
future: True
checkpoint_path: '../../../deepspeechattack/deepspeech.pytorch/checkpoints/clean_notdenoiser_waveform_scalar_True_future_True_future_amt50_firstlayer_False_capped_True_power_0.008_lr_0.00015/check/epoch=02-stepstep=15299.00.ckpt'

data:
  train_path: 'data/libri_train_manifest.json'
  val_path: 'data/libri_val_manifest.json'
  test_path: 'data/libri_test_manifest.json'
  batch_size: 4 # Batch size for training
  num_workers: 4 # Number of workers used in data-loading
  labels_path: 'labels.json'  # Contains tokens for model output
  spect:
    sample_rate: 16000  # The sample rate for the data/model features
    window_size: .02  # Window size for spectrogram generation (seconds)
    window_stride: .01  # Window stride for spectrogram generation (seconds)
    window: 'hamming'
    size: 1.0
    power: 0.5
  augmentation:
    speed_volume_perturb: False  # Use random tempo and gain perturbations.
    spec_augment: False  # Use simple spectral augmentation on mel spectograms.
    noise_dir: ''  # Directory to inject noise into audio. If default, noise Inject not added
    noise_prob: 0.4  # Probability of noise being added per sample
    noise_min: 0.0  # Minimum noise level to sample from. (1.0 means all noise, not original signal)
    noise_max: 0.5

model:
  rnn_type: nn.LSTM  # Type of RNN to use in model
  hidden_size: 1024  # Hidden size of RNN Layer
  hidden_layers: 5
  lookahead_context: 20
  model_path: librispeech_pretrained_v3.ckpt
  mag_noise: True

power: 0.008
future_amt: 50
future_amt_waveform: 8000
residual: False
batchnorm: True
trainer:
  checkpoint_callback: True
  default_root_dir: None
  gradient_clip_val: 5000000000
  process_position: 0
  num_nodes: 1
  num_processes: 8
  gpus: 8
  auto_select_gpus: False
  progress_bar_refresh_rate: 1
  overfit_batches: 0.0
  track_grad_norm: -1
  check_val_every_n_epoch: 1
  fast_dev_run: False
  accumulate_grad_batches: 1
  max_epochs: 1000
  min_epochs: 1
  limit_train_batches: 1.0
  limit_val_batches: 0.0
  limit_test_batches: 1.0
  val_check_interval: 0.0
  flush_logs_every_n_steps: 100
  log_every_n_steps: 50
  accelerator: "ddp"
  sync_batchnorm: False
  precision: 32
  weights_summary: "top"
  weights_save_path: None
  num_sanity_val_steps: 0
  benchmark: False
  deterministic: False
  reload_dataloaders_every_epoch: False
  auto_lr_find: False
  replace_sampler_ddp: True
  terminate_on_nan: False
  auto_scale_batch_size: False
  prepare_data_per_node: True
  amp_backend: "native"
  amp_level: "O2"
  move_metrics_to_cpu: False